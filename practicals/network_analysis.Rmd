---
title: "Network Analysis and Spatial Interaction Models"
author: |
  | 
  | Emmanouil Tranos
  |
  | University of Bristol, Alan Turing Institute 
  | e.tranos@bristol.ac.uk, [\@EmmanouilTranos](https://twitter.com/EmmanouilTranos),  [etranos.info](https://etranos.info/)

output:
  html_document:
    toc: true
    toc_float: true
---

## Resources

Some of the materials for this tutorial have been adapted from:

- the [Origin-destination data with stplanr](https://docs.ropensci.org/stplanr/articles/stplanr-od.html) manual,

- the [Modelling Population Flows Using Spatial Interaction Models](https://rpubs.com/adam_dennett/376877) tutorial, and

- [Oshan (2016)](http://openjournals.wu.ac.at/region/paper_175/175.html)

- [Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. Geocomputation with R. CRC Press, 2019](https://geocompr.robinlovelace.net/)

## Philosophy of this `RMarkdown` document

As you can see this is a long `.Rmd` document, which has a dual objective. On one 
hand, it will help you achieve the unit learning outcomes as it provides an implementation
of most of the concepts and ideas we discuss for this unit. On the other hand this
is almost a representation of a 'real world' workflow. Instead of breaking the code
in shorter and maybe more digestible `.Rmd` documents I decided to provide you with 
a working sequence of all actions I would have taken in order to analyse a network 
with spatial dimensions such as the network of commuting flows in $2011$.
You can use this workflow as the basis for building your own approach and, consequently, 
code in order to complete this session's project. 
We will spend at least $2$ sessions to go through this code.

## Install and load packages

We will use quite a few packages for this tutorial. Most of the code is based on 
the `tidyverse` logic -- see [here](https://www.tidyverse.org/) for more info. But 
the main package we will use for network data wrangling and analysis is 
`igraph` -- more info [here](https://igraph.org/r/).

The below code checks whether these packages are installed in your system and if not it installs them.

```{r setup, include=TRUE, results= 'hide', message=FALSE}
list.of.packages <- c("igraph", "knitr", "corrplot", "corrgram", "rgdal", "tidyverse", "spdplyr", "geojsonio", "stplanr", "leaflet", "SpatialPosition", "stargazer", "DescTools", "patchwork", "caret")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

#library(data.table)
library(igraph)
library(knitr)
library(corrplot)
library(corrgram)
library(rgdal)
library(tidyverse)
library(spdplyr)
library(geojsonio)
library(stplanr)
library(leaflet)
library(SpatialPosition)
library(stargazer)
library(DescTools)
library(patchwork)
library(caret)
```

## Commuting data and networks

For this tutorial we will use travel to work data from the 2011 UK Census. The 
data is available [online](http://wicid.ukdataservice.ac.uk/cider/wicid/downloads.php?wicid_Session=742d61be88b0614c3982455e542bc776), 
but it requires an academic login. After you log in, download the **WU03UK** element, 
save the .csv on your working directory under a `/UK` directory and unzip it. We 
will use the:

> Location of usual residence and place of work by method of travel to work

for

> Census Merged local authority districts in England and Wales, Council areas in Scotland, Local government districts in Northern Ireland.

The below code loads the data.

```{r load, include=TRUE, results= 'hide', message=FALSE}
commuting <- read_csv("UK/wu03uk_v3/wu03uk_v3.csv")
glimpse(commuting)
```

As you may have noticed, the `commuting` object includes only the codes for the 
local authorities. Let's try to see these codes.

First for the origins.

```{r unique_la_o, include=TRUE, results= 'markup', message=FALSE}
commuting %>% distinct(`Area of usual residence`)
```
And the same for the destinations (results omitted). 

```{r unique_la_d, include=TRUE, results= 'hide', message=FALSE}
commuting %>% distinct(`Area of workplace`)
```

```{block, type='alert alert-warning'}
**Question**: Can you guess the countries these codes refer to?
```

You might have observe some weird codes (OD0000001, OD0000002, OD0000003 and OD0000004).
With some simple Google searching we can find the [2011 Census Origin-Destination Data User Guide](https://www.ons.gov.uk/file?uri=/census/2011census/2011censusdata/originanddestinationdata/secureoriginanddestinationtables/2011censusoduserguide_tcm77-383888.pdf), which indicates that these codes do not refer to local authorities:

- OD0000001 = Mainly work at or from home

- OD0000002 = Offshore installation

- OD0000003 = No fixed place

- OD0000004 = Outside UK

For the sake of simplicity we will remove these non-geographical nodes.

```{r drop non_la, include=TRUE, results= 'asis', message=FALSE}
non.la <- c("OD0000001", "OD0000002", "OD0000003", "OD0000004")
commuting <- commuting %>% 
  filter(!`Area of workplace` %in% non.la)
```

check the `%in%` [operator](https://stat.ethz.ch/R-manual/R-devel/library/base/html/match.html).

Now let's do some clean-up of the `commuting` data frame.
Let's remind ourselves how the data look like

```{r la.cleanup, include=TRUE, results= 'markup', message=FALSE}
glimpse(commuting)
```

We are keeping the English and Wales local authorities by keeping the observations 
with a local authority code starting from E (for England) and W (for Wales).

```{r la.cleanup2, include=TRUE, results= 'markup', message=FALSE}
commuting <- commuting %>% filter(startsWith(`Area of usual residence`, "E") |
                                  startsWith(`Area of usual residence`, "W")) %>% 
                           filter(startsWith(`Area of workplace`, "E") |
                                  startsWith(`Area of workplace`, "W")) %>% 
  glimpse()
```

We can also see we many rows we dropped with `glimpse()`.

It is very important to distinguish between intra- and inter-local authority flows.
In network analysis terms, these are the values we find on the diagonal of an
adjacency matrix and refer to the commuting flows within a specific local authority
or between different ones. For this exercise we are dropping the intra-local
authority flows. Although not used here,  we also create a new object with the
intra-local authority flows.

```{r la.cleanup3, include=TRUE, results= 'markup', message=FALSE}
commuting.intra <- commuting %>%
  filter(`Area of usual residence` == `Area of workplace`)
commuting <- commuting %>%
  filter(`Area of usual residence` != `Area of workplace`) %>% 
  glimpse()
```

Please note the constant use of `glimpse()` to keep control of how many observations
we have and check if we missed anything.

Also, take a note of the `commuting` object, which includes multiple types of
commuting flows. Therefore, we will build $3$ different networks:

1. one for all the commuting flows

2. one only for train flows

3. one only for bicycle flows.

```{r include=TRUE, results= 'markup', message=FALSE}

commuting.all <- commuting %>%
  select(`Area of usual residence`,
                `Area of workplace`,
                `All categories: Method of travel to work`) %>%
  rename(o = `Area of usual residence`,     # Area of usual residence is annoyingly
         d = `Area of workplace`,           # long, so I am renaiming theses columns
         weight = `All categories: Method of travel to work`)

# just FYI this is how you could have achieved the same output using base R
# instead of dplyr of the tidyverse ecosystem
# commuting.all <- commuting[,1:3]
# names(commuting.all)[1] <- "o"
# names(commuting.all)[2] <- "d"
# names(commuting.all)[3] <- "weight"

commuting.train <- commuting %>%
  select(`Area of usual residence`,
         `Area of workplace`,
         `All categories: Method of travel to work`,
         `Train`) %>%
  rename(o = `Area of usual residence`,
         d = `Area of workplace`,
         weight = `All categories: Method of travel to work`) %>%
  # The below code drops all the lines with 0 train flows in order to exclude
  # these edges from the network.
  filter(Train!=0)

commuting.bicycle <- commuting %>%
  select(`Area of usual residence`,
                `Area of workplace`,
                `All categories: Method of travel to work`,
                `Bicycle`) %>%
  rename(o = `Area of usual residence`,
         d = `Area of workplace`,
         weight = `All categories: Method of travel to work`) %>%
  # The below code drops all the lines with 0 bicycle flows in order to exclude
  # these edges from the network.
  filter(Bicycle!=0)
```

Unless you know the local authority codes by hard, it might be useful to also add
the corresponding local authority names. These can be easily obtained from the [ONS](https://geoportal.statistics.gov.uk/datasets/ons::census-merged-local-authority-districts-december-2011-generalised-clipped-boundaries-in-great-britain/about).
The below code directly downloads a `GeoJSON` file with the local authorities in
England and Wales. If you don't know what a `GeoJSON` file is, have a look
[here](https://en.wikipedia.org/wiki/GeoJSON).
Boundary data can also be obtained by
[UK Data Service](https://borders.ukdataservice.ac.uk/easy_download.html).

For the time being we are only interested in the local authority names and codes.
We will use the spatial object later.

**Tip**: the below code downloads the `GeoJSON` file over the web. If you want to
run the code multiple times, it might be faster to download the file ones, save it
on your hard drive and the point this location to `readOGR()`.

```{r la1, include=TRUE, results= 'hide', message=FALSE}
la <- readOGR("https://opendata.arcgis.com/datasets/d54f953d633b45f5a82fdd3c89b4c955_0.geojson")#, layer="OGRGeoJSON")

glimpse(la@data) # la@data is the dataframe linked to the spatial object la. 
                 # This is all we need for now.

la.names <- la@data %>% 
  select(cmlad11cd, cmlad11nm) # all we need is the LA names and codes

```

The next step is to actually create the network objects. The below code creates
the `igraph` network objects using the `graph_from_data_frame()` function as we
already have all the necessary data in data frames (`commuting.all`, `commuting.train`
and `commuting.bicycle`). We then attach the local authority names as an attribute
to these networks.

```{r}
net.all <-graph_from_data_frame(commuting.all, directed = TRUE, vertices = la.names)

net.train <-graph_from_data_frame(commuting.train, directed = TRUE, vertices = la.names)

net.bicycle <-graph_from_data_frame(commuting.bicycle, directed = TRUE, vertices = la.names)
```

## Network attributes

The below `igraph` functions illustrate some attributes of the network with all
the flows.

```{r attributes, include=TRUE, results= 'markup', message=FALSE}
# It provides information about the type of the net.all object. Not surprisingly,
# it is an igraph network.
class(net.all)

# It displays the network file, the number of nodes and edges (345 and 92,688
# in this case).
net.all

# It displays the vertices (aka nodes)
V(net.all)

# It displays the vertex attributes. In this case the local authority codes.
vertex_attr(net.all) %>% glimpse()
# the output of vertex_attr() is quite long, this is why I chained it with glimpse()

# It displays the edges.
E(net.all) %>% glimpse()
# the output of E() is quite long, this is why I chained it with glimpse()

# It displays the weights for each edge. In our case the weights represent commuters.
edge.attributes(net.all)$weight %>% glimpse()
# as above re: glimpse()

# Asks whether the network is weighted or not
is.weighted(net.all)
```

```{block, type='alert alert-warning'}
**Question**: How many nodes and edges are there for the other types of networks?
What do these differences mean?
```

## Network measures

The below `igraph` functions calculate some simple network measures.
Have a look at the lecture slides and the reference list to remind yourselves.

```{r network_measures, include=TRUE, results= 'markup', message=FALSE}

# Network diameter. We do not consider the weights because it affects the measurement.
d.all <- diameter(net.all, directed = TRUE, weights = NA)
d.all

# Average path length
mean_ditst.all <- mean_distance(net.all)
mean_ditst.all

# Network density
dens.all <- edge_density(net.all)
dens.all

# Clustering Coefficient or Transitivity
trans.all <- transitivity(net.all)
trans.all

# Reciprocity
rec.all <- reciprocity(net.all)
rec.all

# Assortativity
ass.all <- assortativity_degree(net.all, directed = T)
ass.all
```

```{block, type='alert alert-warning'}
**Question:** Do the same for the other types of commuting networks and compare the different measures.
Why do we observe these differences?
```

## Centralities

Now we are moving from network-level measures to some node-level ones.
Specifically, we will calculate different centrality measures.

```{r centralities, include=TRUE, results= 'markup', message=FALSE}
# Binary in-degree centrality
in.degree <- degree(net.all, mode = "in")
head(in.degree)

# Binary out-degree centrality
out.degree <- degree(net.all, mode = "out")
head(out.degree)

# Binary degree centrality
degree <- degree(net.all, mode = "all")
head(degree)

# The function graph.strength() calculates the weighted degree centrality

# Weighed in-degree centrality
w.in.degree <- graph.strength(net.all, mode = "in")
head(w.in.degree)

# Weighed out-degree centrality
w.out.degree <- graph.strength(net.all, mode = "out")
head(w.out.degree)

# Weighed degree centrality
w.degree <- graph.strength(net.all, mode = "all")
head(w.degree)

# The function betweenness() calculates betweenness centrality. As before
btwnss <- betweenness(net.all, weights = NA)
head(btwnss)

# Eigenvector centrality
eigen <- eigen_centrality(net.all)

# Be careful, eigen has a more complicated structure. Use ?eigen_central to read more
str(eigen)

# We are interested in eigen$vector
head(eigen$vector)

# page rank centrality
prank <- page_rank(net.all, directed = T)
str(prank) # as before
head(prank$vector)
```

Now that we understood how the above work, let's chain them together to create a
`centralities` tibble.

```{r}
centralities <- tibble(
  names = vertex_attr(net.all)[[2]],
  # The above creates a vector with the nodes names (i.e. the local authority names).
  # We are interested in the second elements of the vertex_attr() as the first one 
  # includes the local authority codes. Try str(vertex_attr(net.all)) to see why.
  # the double squared brackets [[]] brings the vertex, while the single one []
  # would have brought a list.
  in.degree = degree(net.all, mode = "in"),
  out.degree = degree(net.all, mode = "out"),
  degree = degree(net.all, mode = "in"),
  w.in.degree = graph.strength(net.all, mode = "in"),
  w.out.degree = graph.strength(net.all, mode = "out"),
  w.degree = graph.strength(net.all, mode = "all"),
  btwnss = betweenness(net.all, weights = NA),
  eigen = eigen_centrality(net.all)$vector,  # note the $vector
  prank = page_rank(net.all, directed = T)$vector) %>% 
  glimpse()
```

Or, if you want a nicer table, you can use `kable()`. 
**Tip** check out the `kableExtra` package for more options 

```{r}
centralities %>% kable(n=10, caption = "Centralities")
```

```{block, type='alert alert-warning'}
**Question**: Can you try to interpret these different centrality measures in the context of our data?
```

This is helpful, but we might also be interested in discussing the rankings: 
*Which one is the most central local authority in the commuting network?* 
Instead of reading from the table, we can just calculate the ranks.

To begin with, let's do a test.

```{r centralities_ranks, include=TRUE, results= 'markup', message=FALSE}

test <- centralities %>%
  mutate(rank.test = dense_rank(desc(in.degree))) %>% # we are interested in dense ranking:
                                                      # i.e. two lines with the same value have
                                                      # will the same ranking,
                                                      # desc stands for descending order
  arrange(rank.test) %>%                              # arranges the data frame based on rank.test
  glimpse()
```

```{r centralities_ranks_table, include=TRUE, results= 'markup', message=FALSE}

ranks <- centralities %>%
  mutate_at(vars(in.degree:prank), 
            funs(dense_rank(desc(.)))) # . for all the selected variables

# Adds a prefix r_ before each column name to indicate the ranks
colnames(ranks) <- paste("r", colnames(ranks), sep = "_")


```

```{block, type='alert alert-warning'}
**Question**: Can you quickly compare the `ranks` with the `cetrnalities` object
based on the rankings?
```

```{r centralities_ranks_table2, include=TRUE, results= 'markup', message=FALSE}

head(centralities)
head(ranks)

# So, because both dataframes have the same structure and order we can just use
# cbind().

centralities <- cbind(centralities, ranks) %>% 
  arrange(w.in.degree) %>% 
  select(-r_names) %>% 
  glimpse()

# It combines the centralities (centralities) and ranks (ranks) objects by columns.
# You can imagine it as stacking the columns of ranks after the columns of centralities.
# Since both objects refer to the same observations (i.e. the same rows), we can
# just combine them.


# And this is a nicer table of centralities:
centralities %>% kable(caption = "Centralities")
```

## Compare centralities
The below code provides a corregram of the different centrality measures.

```{r corrgram, include=TRUE, results= 'markup', message=FALSE}
cor.mat <- cor(centralities[,c(2:10)])
corrplot(cor.mat, type="upper")
```

```{block, type='alert alert-warning'}
**Question:**
Discuss the differences between the different centrality measures in the context
of the commuting network.
```

## Community detection

To begin with, we need to convert our network to an undirected one, as the `fast_greedy`
algorithm we are using can only be applied to such networks.

```{r communities, include=TRUE, results= 'markup', message=FALSE}

net.all.und <- as.undirected(net.all,
                             mode=c("mutual"),
                             edge.attr.comb = igraph_opt("edge.attr.comb"))

communities.net.all <- cluster_fast_greedy(net.all.und)

# This provides a summary of the community algorithm:
print(communities.net.all)

# To see how many communities we have, run the below:
length(communities.net.all)

# And these are the community sizes:
sizes(communities.net.all)

# Now, let's create a new object with the community membership
#communities.net.all_membership <- membership(communities.net.all)

# And convert it to a data.frame

communities.net.all_membership <- membership(communities.net.all) %>%
  unclass %>%                          # we first need to unclass the object
  as.data.frame %>%                    # we convert it to a dataframe
  rename(community = ".") %>%          # rename the community column
  rownames_to_column("cmlad11cd") %>%  # we 'move' the rownames to a
                                       # new column in order to do a merge below
  left_join(la.names) %>%              # and now we can merge it with the LA names
  arrange(community) %>%               # arrange based on the community membership
  glimpse()

# this is just a test to see if the `left_join` led to any NAs
sapply(communities.net.all_membership, function(x) sum(is.na(x)))
```

Let's try now to map our output. We need the local authorities shape file we have
already loaded.

```{r communities mapping, include=TRUE, results= 'markup', message=FALSE}

head(la)
la$geo_labelw <- NULL
la <- la[,c(2,1)]
la$objectid <- NULL

# Before we do any further analysis we make sure to convert the la object to WGS84
# Longitude / Latitude Coordinate System.
wgs84 = '+proj=longlat +datum=WGS84'
la <- spTransform(la, CRS(wgs84) )

# This is the @data element of the la spatial object
head(la@data)

# # Let's merge it with the communities data frame
# la@data <- merge(la@data, communities.net.all_membership)
# la@data$community <- as.factor(la@data$community)

# We fortify using the tidy function from broom package.
la.f <- broom::tidy(la, region = "cmlad11cd")
# Have a look at the la.f object to see what tidy() does

# And merge again
la.f <- merge(la.f, communities.net.all_membership, by.x = "id", by.y = "cmlad11cd")

# Please note that I used base R for the above. I could have easily used the dplyr
# equivalent. Which function would this be?

# just a check to see how the merge worked
sapply(la.f, function(x) sum(is.na(x))) 

# And this is our map
ggplot(la.f, aes(long, lat, group = group, fill = as.factor(community))) + geom_polygon(colour = "black") +
  coord_equal() +
  ggtitle("Communities using the 'fast and greedy' algorithm")
```

```{block, type='alert alert-warning'}
**Question**: What do you think about the output? Can we learn anything? Can you try different community algorithms?
Check [igraph's manual](https://igraph.org/r/doc/)  and [Javed et al (2018)](https://www.sciencedirect.com/science/article/pii/S1084804518300560)
```

## Network Visualisation

The below chunks of code offer some intro to plotting network data.
Have a look to see how the code works.

```{r vis, include=TRUE, results= 'markup', message=FALSE}
plot(net.all, # the graph to be plotted
     layout=layout.fruchterman.reingold, # the layout method. see the igraph documentation for details
     main='My first graph in R', # specifies the title
     vertex.label.dist=0.5, # puts the name labels slightly off the dots
     vertex.frame.color='blue', # the colour of the border of the dots
     vertex.label.color='black', # the colour of the name labels
     vertex.label.font=2, # the font of the name labels
     vertex.label=V(net.all)$id, # specifies the labels of the vertices. in this case the 'name' attribute is used
     vertex.label.cex=.5,	# specifies the size of the font of the labels. can also be made to vary
     edge.arrow.size=0.1) # specifies the arrow size
```

Not a very nice outcome :(
Let's remove some information

```{r vis2, include=TRUE, results= 'markup', message=FALSE}
plot(net.all, # the graph to be plotted
     layout=layout.fruchterman.reingold, # the layout method. see the igraph documentation for details
     main='My second graph in R',	# specifies the title
     vertex.frame.color='blue', # the colour of the border of the dots
     vertex.label.font=2,	# the font of the name labels
     vertex.label=NA,	# no labels for the vertices
     edge.arrow.size=0, # specifies the arrow size
     vertex.size=5) # vertex size
```

Let's try to only plot nodes with high weighted degree centrality.

```{r vis3, include=TRUE, results= 'markup', message=FALSE}

# The below gives us the 90th percentile:
q <- quantile(strength(net.all), .9)

# Then, we create a network with the nodes we DON'T want to plot. In this case the lowest 90%
low_nodes <- V(net.all)[strength(net.all) < q] # 108659 is the 90% percentile. Feel free to play with different numbers

# And this is the network with the 10% of the most central nodes
net.all.central <- delete.vertices(net.all, low_nodes)

# The below uses the node degree centrality to plot the node size.
# Pay attention to the 0.0001 factor.

plot(net.all.central, # the graph to be plotted
     layout=layout.fruchterman.reingold, # the layout method. see the igraph documentation for details
     main='My third graph in R', # specifies the title
     vertex.frame.color='blue', # the colour of the border of the dots
     vertex.label.font=2,	# the font of the name labels
     vertex.label=V(net.all.central)$id, # no labels for the vertices
     vertex.label.font=1, # the font type of the name labels (1 plain, 2 bold, 3, italic, 4 bold italic, 5 symbol)
     vertex.label.cex=.5,	# specifies the size of the font of the labels. can also be made to vary
     edge.arrow.size=0, # specifies the arrow size
     vertex.size=strength(net.all.central)*0.0001) # defines the node size based on weighted degree centrality
```

Not very nice either...

```{block, type='alert alert-warning'}

For the next time:

1. spend some time browsing the `igraph`'s manual,

2. search for code online to `plot large netwoks in R using igraph`, and

3. use the following tutorials

  - [netVizR](http://mr.schochastics.net/netVizR.html)
(the data can be found [here](http://mr.schochastics.net/#projects) under the
'Network Visualization in R' section

  - [edge-bundling](http://blog.schochastics.net/post/non-hierarchical-edge-bundling-in-r/)

in order to produce more meaningful network visualisations of the UK commuting
network.

I am interesting in one or multiple plots with:

- all or a subset of the nodes and edges.
How could you select such a subset?

- the communities you detected.

- varying size of nodes based on a centrality measure. Again, you can
decide to plot a subset of the network based on some network characteristics.

- an *appropriate* to the network layout.
```

## Mapping
Until now we focused mostly on the topology of the network and we ignored its spatial
dimension. However, this is an important attribute of the commuting network and
we should necessarily consider it and incorporate it in our analysis. To begin with
we will map these commuting flows. Bare in mind that this is not a trivial process
as, in essence, we need to attach the geographical coordinates to the network nodes
and plot the network based on these coordinates. Given the size of our network this
might be computationally expensive. It is also challenging to create a meaningful
map given the size of the network.


The local authorities spatial object is necessary in order to use the `od2line()`
function from the `stplanr` package. This is a very useful function transforms
origin to destination (OD) tables to linear spatial objects. In order for this function
to work we need the above spatial object with the zones of the origin and destination
flows. Importantly it needs to only include the zone codes (i.e. the local authority
codes), which should match with the origin and destination codes.This is the `la`
spatial object of the local authorities in England and Wales, which has already
been loaded in R.

The below code just plots the boundaries of local authorities.

```{r la2, include=TRUE, results= 'markup', message=FALSE}
ggplot(la.f, aes(long, lat, group = group)) + geom_polygon(colour = "black", fill = "white") +
  coord_equal() +
  ggtitle("Local authorities")
```

Now let's do some clean-up of the `commuting.all` data frame in order to convert
it to a spatial object using the `od2line()` function.

```{r lod2line, include=TRUE, results= 'markup', message=FALSE}
# We start by plotting a histogram of the data. We are using ggplot() as the output
# is nicer, but the simpler hist() could also be used

options(scipen=999) # It prevents R from using scientific notation for numbers
ggplot(commuting.all,
       aes(x=weight)) +
  geom_histogram() +
  geom_vline(aes(xintercept=mean(weight)), # This line of code adds a vertical line to represent the mean
             color="blue", linetype="dashed", size=1)
```

As you can see the flow data is very skewed. So, let's truncate the data and plot
a histogram for flows between origin and destinations will less than 1000 people.

```{r lod2line2, include=TRUE, results= 'markup', message=FALSE}
ggplot(commuting.all[commuting.all$weight < 1000,],
       aes(x=weight)) +
  geom_histogram() +
  geom_vline(aes(xintercept=mean(weight)),
             color="blue",
             linetype="dashed",
             size=1)
```

There is a very large number of local authority pairs with very few people commuting
between these OD pairs. So, in order to decrease the data and the mapping complexity
we will keep only the OD pairs with more than 5 commuters.

```{r truncate, include=TRUE, results= 'markup', message=FALSE}

commuting.all.truncated <- commuting.all %>% 
  filter(weight > 5) %>% 
  glimpse()
```

```{block, type='alert alert-warning'}
**Question:**
How many limes did we filter out?
```

The next chunk of code converts the origin-destination table to a spatial object using
the corresponding zones spatial object (la) that we have already loaded.

```{r lod2line3, include=TRUE, results= 'markup', message=FALSE}

# od2line() has some strict requirement regarding data structure. 
# Check ?od2line
# Therefore, we kept only the local authority code and name.
# The actual requirement is to have the the zone code as the 
# first column of the zone object. 
# If you remember, we run the following when we loaded the la object: 
# la@data <- la@data %>% 
#   select(cmlad11cd, cmlad11nm)


travel_network <- od2line(flow = commuting.all.truncated, 
                          zones = la)

# As before, we transform the new spatial object to WGS84.
travel_network <- spTransform(travel_network, CRS(wgs84))

# Just to see the class of the new spatial object
class(travel_network)

plot(travel_network)
```
This is a naive plot of the spatial network. We can guess the shape of England and
Wales, but this is like a *hairball* network and far from being useful. In order
to produce a more useful visualisation we will employ the `leaflet`
[package](https://rstudio.github.io/leaflet/), which produces interactive maps
using `JaveScript` Libraries.

```{r leaflet, include=TRUE, results= 'markup', message=FALSE}
# this is the colour pallet we are going to use based on 5 quantiles
Flow_pal <- colorQuantile("YlOrBr", domain = travel_network$weight, n=5)

leaflet() %>%
  addTiles() %>%
  addPolylines(
    data = travel_network,
    weight = .1,
    color = ~Flow_pal(weight),
    opacity = .6) %>%
    addLegend("topright",
              pal = Flow_pal,
              values = travel_network$weight,
              title = "Commuting flows in 2011")
```

Let's try now to create an interactive map showing only the flows originating from
Bristol and Birmingham.

```{r leaflet2, include=TRUE, results= 'markup', message=FALSE}

# To begin with let's find out the local authority codes for Bristol and Birmingham
# Look up the grep() function. Very useful!
la.names[grep("Bristol", la.names$cmlad11nm),]
la.names[grep("Birmingham", la.names$cmlad11nm),]

# These are E41000023 and E41000281 respectively.

# Next we will create the 'groups' of local authorities
travel_network$la.groups = "Rest"
travel_network$la.groups[travel_network$o == "E41000023"] = "Bristol"
travel_network$la.groups[travel_network$o == "E41000281"] = "Birmingham"

leaflet() %>%
  addTiles() %>%
  addPolylines(
    data = travel_network,
    weight = 1, # Notice the different value for better visual effect when zoom in
    color = ~Flow_pal(weight),
    opacity = .8, # as above
    group = travel_network$la.groups) %>%
  addLayersControl(
    position = "bottomleft",
    overlayGroups = unique(travel_network$la.groups),
    options = layersControlOptions(
      collapsed = FALSE)) %>%
  addLegend("topright",
          pal = Flow_pal,
          values = travel_network$weight,
          title = "Commuting flows in 2011")
```

## Spatial modelling

Let's now move to model these flows. If you remember the basic Gravity model is
defined as following:

$$T_{ij} = k \displaystyle \frac{V_i^\lambda M_j^\alpha}{D_{ij}^\beta}$$

And if we take the logarithms of both sides of the equation we can transform the
Gravity model to something which looks like a linear model:

$$lnT_{ij} = lnk + \lambda lnV_i + \alpha lnM_j - \beta lnD_{ij}$$

The above transformed equation can be estimated as a linear model if we assume that
$y = lnT_{ij}$, $c = lnk$, $x_1 = lnV_i$, $a_1 = \lambda$ etc. Hence, we can use
OLS to estimate the following:

$$lnT_{ij} = lnk + \lambda lnV_i + \alpha lnM_j - \beta lnD_{ij} + e_{ij}$$

This is what is known as the log-linear transformation.

There are a number of issues with such an approach though. Most importantly, our
dependent variable is not continuous, but instead a discrete, positive variable
(there are no flows of -324.56 people!). Therefore we need to employ an appropriate
estimator and this is what the Poisson regression does. Briefly, if we exponentiate
both sides, the above equation can be written as:

$$T_{ij} = e^{lnk + \lambda lnV_i + \alpha lnM_j - \beta lnD_{ij}}$$
The above is in the form of the Poisson regression. So, we are interested in modelling
*not* the mean $T_{ij}$ drawn from a normally distributed $T$, but, instead, the
mean $T_{ij}$, which is the average of the all the flows (i.e. counts) between any
$i$ and $j$. For more details about the Poisson regression have a look at the
(Legler and Roback 2019)[https://bookdown.org/roback/bookdown-bysh/ch-poissonreg.html#initial-models].
In total, we will estimate the commuting flows using both OLS and Poisson regressions.

But before we get into the estimation we need to build a data frame, which includes
all the necessary variables. These are the origin-destination flows $T$ between
$i$ and $j$, the distance $dist$ between $i$ and $j$ and the characteristics $V$
and $M$ of origin $i$ and destinations $j$ that we believe *push* and *pull* individuals
to commute.

```{r si data, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

# we use the `SpatialPosition` package and the `CreateDistMatrix()` function to
# calculate a distance matrix for all local authorities,
la.d <- CreateDistMatrix(la, la, longlat = T)

# we use as column and row name the local authority codes
rownames(la.d) <- la$cmlad11cd
colnames(la.d) <- la$cmlad11cd

# This is a matrix of the distances between *all* local authorities. We then use
# the function as.data.frame.table() to convert this matrix to a data frame each
# line of which represents an origin-destination pair.
la.d <- as.data.frame.table(la.d, responseName = "value")
glimpse(la.d)

# Please note that the elements of the diagonal are present in this distance matrix.
```

If you want to check that the distances we are correct, use google maps to calculate
the distance between E41000001 (Middlesbrough) and E41000002 (Hartlepool).
Remember that the la.d is expressed in meters.

What is missing here? Do you remember the intra-zone commuting flows?
E.g. the commuters that live and work in Bristol? We had removed these from the
network analysis and visualisation element. Because we don't have information about
the distances of the intra-zone commutes, we will exclude them from this analysis.

```{r si data2, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
la.d <- la.d %>%
  filter(Var1 != Var2) %>% 
  glimpse()
```

What we want to do is to match the data frame with all the distances with the
commuting flows data frame. To do that we will (1) create a new code for the
origin-destination pair and (2) match the two data frames.

```{r si data3, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

la.d <- la.d %>% 
  mutate(ij.code = paste(Var1, Var2, sep = "_")) %>% 
  glimpse()

commuting <- commuting %>% 
  mutate(ij.code = paste(`Area of usual residence`,
                          `Area of workplace`,
                           sep = "_")) %>% 
  glimpse()
```

Before we perform the match, keep a note of how many observations both data frames 
have in order to check if we loose any observations during the matching
As you can see the commuting data frame has less observations than the la.d one,
which includes all the possible origin-destination pairs. 

What does it mean?
That for some origin-destination pairs there are no commuting flows, which of
course makes sense. We need to include these pairs with $flow = 0$ in our data though
because the lack of commuting flows **is not** missing data!

```{r si data4, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

commuting.si <- full_join(la.d, commuting, by = "ij.code") %>% 
  glimpse()

# Some variables are repetitive or need name change
commuting.si <- commuting.si %>% 
  rename(i = Var1,
         j = Var2,
         distance = value) %>% 
  select(-`Area of usual residence`,
         -`Area of workplace`) %>% 
  glimpse()

# Let's see if we have any missing values
sapply(commuting.si, function(x) sum(is.na(x)))
```

There are quite a few. What does it mean? That there are no commuting flows for
these origin-destination pairs and,therefore, were excluded from the origin
commuting data set we downloaded. So, we are going to replace these $NAs$ with
$0s$.

```{r si data5, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
commuting.si <- commuting.si %>% 
  replace(., is.na(.), 0)
```

Now let's bring data for some $i$ and $j$ characteristics that we believe that
affect commuting. I have prepared such a small data set from the census, which
includes resident population and working populations as *push* and *pull* variables.
These data have been obtained by [nomis](https://www.nomisweb.co.uk/published/census/odexplorer.asp).

```{r si ij data, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}
data.workplace <- read.csv("https://www.dropbox.com/s/0ym88p8quwaiyau/data_workplace.csv?dl=1")

# Dropbox trick: to use in an .Rmd the link that Dropbox provides to share a file
# replace dl=0 with dl=1 at the end of the link

data.resident <- read.csv("https://www.dropbox.com/s/09d7v5cm6ov3ioz/data_resident.csv?dl=1")

commuting.si <- commuting.si %>% 
  left_join(data.resident, by = c('i' = 'Merging.Local.Authority.Code')) %>% 
  left_join(data.workplace, by = c('j' = 'Merging.Local.Authority.Code')) 
  
# commuting.si <- merge(commuting.si, data.resident, by.x = "i",
#                        by.y = "Merging.Local.Authority.Code",
#                        all.x = TRUE)
# 
# commuting.si <- merge(commuting.si, data.workplace, by.x = "j",
#                        by.y = "Merging.Local.Authority.Code",
#                        all.x = TRUE)
```

```{block, type='alert alert-warning'}
**Question** are there any redundant columns? Can you remove them?
```

Before we start modelling these flows, let's plot our variables.

```{r si.plots, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
ggplot(commuting.si, aes(x=distance,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)

ggplot(commuting.si, aes(x=resident,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)

ggplot(commuting.si, aes(x=workplace,
                         y=`All categories: Method of travel to work`)) +
  geom_point() +
  geom_smooth(method=lm)
```

```{block, type='alert alert-warning'}
**Question** What do you take from these graphs?
```

Let's try now to model these flows. We will start with a simple OLS to estimate
the above specifications. Please pay attention to the small trick we did. Because
there are non-materialised origin-destination pairs (i.e. with 0 flows), we added
a small value ($0.5$) in the dependent variable. Otherwise we will receive an
error as the logarithm of $0$ is not defined.

```{r si_ols, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

ols.model <- lm(log(`All categories: Method of travel to work`+.5) ~
                  log(distance) + log(resident) + log(workplace),
                data = commuting.si)

# To see the model output you can use the summary() function.
summary(ols.model)
```

So, the OLS regression estimated the four parameters:

- $lnk = 7.971$
- $\beta = -1.840$
- $\lambda = 0.514$
- $\alpha = -0.852$

Let's estimate now our model using a Poisson regression. Given that we don't take
the logarithm of the dependent variable, there is no need to add $0.5$.

```{r si_poisson, include=TRUE, results= 'markup', message=FALSE, warning = FALSE}

glm.model <- glm((`All categories: Method of travel to work`)~
                   log(distance) + log(resident) + log(workplace),
                 family = poisson(link = "log"), data = commuting.si)
summary(glm.model)
```

The following parameter1 have been estimated

- $lnk = 11.635$
- $\beta = -1.816$
- $\lambda = 0.319$
- $\alpha = 0.820$

As you can see the differences are rather small.

The `stargazer` package I use below creates elegant regression tables.
Replace `type = "html"` with `type = "text"` to have be able to read the results
using the .Rmd document. the "html" option is useful for when knitting the script
to an .html document.

```{r stargazer, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
stargazer(ols.model, glm.model, type = "html")
```

```{block, type='alert alert-warning'}
**Question** Can you interpret the regression results?
```

## This is not a Machine Learning introduction...

... but maybe a sneak preview of the philosophy behind *modern* data science approaches
to answer computational problems. We are using the `caret` packahe to

- split our data into test and training

- train a lm()` and `glm()` model using the training data set

- use the estimated models to make out-of-sample predictions using the test data set

- plot the results and the relevant metrics and choose the best model.

This is obviously an oversimplification with quite a few errors, but it provides
a good preview of machine learning frameworks. 

It is worth browsing the [`caret`](https://topepo.github.io/caret/) package.

```{r predict, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}
library(caret)

commuting.si <- commuting.si %>% 
  drop_na() %>% 
  glimpse()
  
set.seed(3456)
trainIndex <- createDataPartition(commuting.si$`All categories: Method of travel to work`, p = .8, 
                                  list = FALSE, 
                                  times = 1)
dim(trainIndex)
dim(commuting.si)

commuting.si.train <- commuting.si[ trainIndex,]
commuting.si.test  <- commuting.si[-trainIndex,]

model.lm <- train(`All categories: Method of travel to work` ~
                    distance + resident + workplace,
                    data = commuting.si,
                    method = "lm")

predictions.lm <- predict(model.lm, commuting.si.test)
lm.metrics <- postResample(pred = predictions.lm, obs = commuting.si.test$`All categories: Method of travel to work`) 

model.glm <- train(`All categories: Method of travel to work` ~
                    distance + resident + workplace,
                    data = commuting.si,
                    method = "glmnet",
                    family = "poisson", 
                    na.action = na.omit)

predictions.glm <- predict(model.glm, commuting.si.test)
glm.metrics <- postResample(pred = predictions.glm, obs = commuting.si.test$`All categories: Method of travel to work`) 
bind_rows(lm.metrics, glm.metrics) %>% 
  mutate(Model = c("lm", "glm")) %>% 
  select(Model, Rsquared, RMSE, MAE) %>% 
  kable(digits = 3)
```

```{block, type='alert alert-warning'}
**Question** Which model would you choose?
```

```{r predict.plots, include=TRUE, results= 'asis', message=FALSE, warning = FALSE}

predict.lm.plot <- bind_cols(commuting.si.test$`All categories: Method of travel to work`, predictions.lm) %>% 
  rename(y = '...1',
         y_hat = '...2') %>% 
  ggplot(aes(x=y, y=y_hat)) + geom_point() + ggtitle("lm") + 
  theme(plot.title = element_text(hjust = 0.5))

predict.glm.plot <- bind_cols(commuting.si.test$`All categories: Method of travel to work`, predictions.glm) %>% 
  rename(y = '...1',
         y_hat = '...2') %>% 
  ggplot(aes(x=y, y=y_hat)) + geom_point() + ggtitle("glm") + 
  theme(plot.title = element_text(hjust = 0.5))

library(patchwork)
predict.lm.plot + predict.glm.plot

```

